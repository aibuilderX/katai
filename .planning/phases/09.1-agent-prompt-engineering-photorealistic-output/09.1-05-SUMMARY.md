---
phase: 09.1-agent-prompt-engineering-photorealistic-output
plan: 05
subsystem: ai-prompts
tags: [evaluation, test-briefs, rubrics, naive-prompts, a-b-comparison, japanese-advertising]

# Dependency graph
requires:
  - phase: 09.1-agent-prompt-engineering-photorealistic-output (plans 01-04)
    provides: Shared modules (frameworks, register-map, platform-norms, flux-techniques, quality-checklist) and 5 optimized agent prompt builders
provides:
  - 8 test briefs (5 standard + 3 edge case) for reproducible evaluation
  - Per-agent evaluation rubrics with 1-5 scoring and concrete anchors
  - Naive prompt baselines for all 5 agents for A/B comparison
  - Complete evaluation infrastructure enabling measurable prompt quality improvement
affects: [future evaluation runs, prompt iteration cycles, phase 10 auto-mode testing]

# Tech tracking
tech-stack:
  added: []
  patterns: [typed test brief definitions with CampaignBrief conformance, structured rubric scoring with anchors, v1.0 prompt mirroring for baseline comparison]

key-files:
  created:
    - src/lib/ai/prompts/evaluation/test-briefs.ts
    - src/lib/ai/prompts/evaluation/rubrics.ts
    - src/lib/ai/prompts/evaluation/naive-prompts.ts
  modified: []

key-decisions:
  - "Test briefs use full N8nWebhookPayload brandProfile shape for maximum pipeline compatibility"
  - "Edge cases test 3 specific failure modes: sparse input (vague brief), contradictions (luxury+budget), emotional sensitivity (pet memorial)"
  - "Naive Copywriter/Art Director mirror actual v1.0 code for fair baseline comparison"
  - "Rubric dimensions mapped to research-identified criteria per agent (not generic quality measures)"
  - "0.5-point improvement threshold on 5-point scale for optimization success criteria"

patterns-established:
  - "TestBrief type: id + name + description + brief + brandProfile + testIntent for self-documenting test data"
  - "Rubric structure: AgentRubric -> RubricDimension[] -> ScoringAnchor[1-5] with concrete examples at each level"
  - "Naive prompt pattern: mirror existing v1.0 code for Copywriter/Art Director; create minimal generic prompts for new agents"

requirements-completed: [ORCH-04, ORCH-05, ORCH-06, ORCH-07, ORCH-08, ORCH-09, ORCH-13, ORCH-14, GENX-09]

# Metrics
duration: 8min
completed: 2026-02-19
---

# Phase 9.1 Plan 05: Evaluation Infrastructure Summary

**8 test briefs (5 standard Japanese small businesses + 3 edge cases), per-agent rubrics with 1-5 scoring anchors, and naive prompt baselines mirroring v1.0 code for A/B comparison**

## Performance

- **Duration:** 8 min
- **Started:** 2026-02-19T08:09:53Z
- **Completed:** 2026-02-19T08:18:13Z
- **Tasks:** 2
- **Files created:** 3

## Accomplishments
- 8 typed test briefs covering cosmetics, food, fashion, nail salon, online course (standard) plus vague, conflicting, and pet memorial (edge cases)
- Evaluation rubrics for all 5 agents with concrete 1-5 scoring examples per dimension (16 dimensions total)
- Naive prompt baselines for all 5 agents -- Copywriter and Art Director directly mirror v1.0 code from copy-generation.ts and image-generation.ts

## Task Commits

Each task was committed atomically:

1. **Task 1: Create test briefs covering 5 standard categories and 3 edge cases** - `e762e20` (feat)
2. **Task 2: Create evaluation rubrics and naive prompt baselines for A/B comparison** - `5d0637d` (feat)

## Files Created/Modified
- `src/lib/ai/prompts/evaluation/test-briefs.ts` - 8 test briefs with CampaignBrief conformance, brand profiles, and documented test intents
- `src/lib/ai/prompts/evaluation/rubrics.ts` - Per-agent rubric definitions with 1-5 scoring anchors and 0.5-point improvement threshold
- `src/lib/ai/prompts/evaluation/naive-prompts.ts` - Baseline naive prompt builders for A/B comparison (5 agents, Copywriter/Art Director mirror v1.0)

## Decisions Made
- Test briefs use `N8nWebhookPayload["brandProfile"]` type directly (not a custom subset) to ensure full pipeline compatibility when running test briefs through the actual agent system
- Edge case briefs specifically test: sparse input handling (vague brief has empty targetAudience and creativeMoodTags), contradiction resolution (LUXE BUDGET has luxury toneDescription but casual registerOverride), and emotional sensitivity (pet memorial requires non-promotional tone)
- Naive Copywriter mirrors the exact `buildSystemPrompt` + `buildCopyPrompt` logic from copy-generation.ts; naive Art Director mirrors `buildImagePrompt` from image-generation.ts -- ensuring fair baseline comparison against actual v1.0 code, not synthetic simplified versions
- Rubric dimensions derived from research section 8.1 and 8.2 criteria per agent, not generic "quality" measures

## Deviations from Plan

None - plan executed exactly as written.

## Issues Encountered
None.

## User Setup Required

None - no external service configuration required.

## Next Phase Readiness
- Phase 9.1 is now complete: all 5 plans executed (shared modules, 4 agent prompt builders, evaluation infrastructure)
- Evaluation framework is ready for A/B comparison runs: pass any of the 8 test briefs through both naive and optimized prompt builders, score outputs against the rubrics
- All 3 evaluation modules (test-briefs, rubrics, naive-prompts) type-check cleanly and integrate with existing pipeline types

## Self-Check: PASSED

- [x] src/lib/ai/prompts/evaluation/test-briefs.ts EXISTS
- [x] src/lib/ai/prompts/evaluation/rubrics.ts EXISTS
- [x] src/lib/ai/prompts/evaluation/naive-prompts.ts EXISTS
- [x] Commit e762e20 EXISTS (Task 1: test briefs)
- [x] Commit 5d0637d EXISTS (Task 2: rubrics + naive prompts)
- [x] npx tsc --noEmit passes with zero errors

---
*Phase: 09.1-agent-prompt-engineering-photorealistic-output*
*Completed: 2026-02-19*
