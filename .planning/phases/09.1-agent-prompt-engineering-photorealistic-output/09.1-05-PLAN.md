---
phase: 09.1-agent-prompt-engineering-photorealistic-output
plan: 05
type: execute
wave: 3
depends_on: [09.1-02, 09.1-03, 09.1-04]
files_modified:
  - src/lib/ai/prompts/evaluation/test-briefs.ts
  - src/lib/ai/prompts/evaluation/rubrics.ts
  - src/lib/ai/prompts/evaluation/naive-prompts.ts
autonomous: true
requirements: [ORCH-04, ORCH-05, ORCH-06, ORCH-07, ORCH-08, ORCH-09, ORCH-13, ORCH-14, GENX-09]

must_haves:
  truths:
    - "5 standard test briefs cover cosmetics, food/restaurant, fashion, personal service (nail salon), and online course — all as Japanese small businesses"
    - "2-3 edge case briefs exist: vague brief (minimal info), conflicting brief (luxury+budget), unusual category (pet memorial)"
    - "Naive prompt baselines exist for all 5 agents — these represent Phase 9 default prompts before optimization"
    - "Evaluation rubrics define 1-5 scoring per dimension per agent with concrete examples of what each score means"
    - "All test briefs include the full CampaignBrief structure with platforms, audience, objective, and product info"
    - "Rubrics map to the specific evaluation criteria from research (Strategic Insight: classification accuracy; Copywriter: naturalness, register, persuasiveness; Art Director: prompt specificity, realism techniques; JP Localization: critique accuracy)"
  artifacts:
    - path: "src/lib/ai/prompts/evaluation/test-briefs.ts"
      provides: "5 standard + 3 edge case test brief definitions"
      exports: ["STANDARD_TEST_BRIEFS", "EDGE_CASE_BRIEFS", "ALL_TEST_BRIEFS"]
    - path: "src/lib/ai/prompts/evaluation/rubrics.ts"
      provides: "Per-agent evaluation rubric definitions with 1-5 scoring"
      exports: ["EVALUATION_RUBRICS", "AGENT_RUBRIC_DIMENSIONS"]
    - path: "src/lib/ai/prompts/evaluation/naive-prompts.ts"
      provides: "Baseline naive prompt builders for A/B comparison"
      exports: ["buildNaiveStrategicInsightPrompt", "buildNaiveCreativeDirectorPrompt", "buildNaiveCopywriterPrompt", "buildNaiveArtDirectorPrompt", "buildNaiveJpLocalizationPrompt"]
  key_links:
    - from: "src/lib/ai/prompts/evaluation/test-briefs.ts"
      to: "src/types/campaign.ts"
      via: "Test briefs conform to CampaignBrief type"
      pattern: "CampaignBrief"
    - from: "src/lib/ai/prompts/evaluation/naive-prompts.ts"
      to: "src/lib/ai/prompts/copy-generation.ts"
      via: "Naive Copywriter prompt mirrors existing v1.0 prompts"
      pattern: "buildSystemPrompt.*buildCopyPrompt"
    - from: "src/lib/ai/prompts/evaluation/rubrics.ts"
      to: "All agent prompt builders"
      via: "Rubric dimensions map to each agent's output quality criteria"
      pattern: "strategicInsight.*creativeDirector.*copywriter.*artDirector.*jpLocalization"
---

<objective>
Create the evaluation infrastructure for A/B comparison between naive and optimized prompts: test briefs, evaluation rubrics, and naive prompt baselines.

Purpose: Without evaluation infrastructure, there is no way to prove that the optimized prompts from Plans 02-04 are measurably better than naive prompts. This plan creates the test data and scoring methodology that enables A/B comparison across all 5 agents, fulfilling the phase goal of "demonstrating measurably superior output compared to naive prompting."

Output: 3 evaluation modules — test briefs (5 standard + 3 edge), rubrics (per-agent scoring), naive baselines (5 agents).
</objective>

<execution_context>
@/Users/hani/.claude/get-shit-done/workflows/execute-plan.md
@/Users/hani/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09.1-agent-prompt-engineering-photorealistic-output/09.1-CONTEXT.md
@.planning/phases/09.1-agent-prompt-engineering-photorealistic-output/09.1-RESEARCH.md
@src/types/campaign.ts
@src/types/pipeline.ts
@src/lib/ai/prompts/copy-generation.ts
@src/lib/ai/prompts/image-generation.ts
@src/lib/ai/prompts/agents/strategic-insight.ts
@src/lib/ai/prompts/agents/creative-director.ts
@src/lib/ai/prompts/agents/copywriter.ts
@src/lib/ai/prompts/agents/art-director.ts
@src/lib/ai/prompts/agents/jp-localization.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test briefs covering 5 standard categories and 3 edge cases</name>
  <files>
    src/lib/ai/prompts/evaluation/test-briefs.ts
  </files>
  <action>
Create `src/lib/ai/prompts/evaluation/test-briefs.ts` exporting typed test brief definitions.

Each test brief must conform to the `CampaignBrief` type from `src/types/campaign.ts`. Also include a `testBrandProfile` for each (matching the brand profile structure from the webhook payload).

**5 Standard Test Briefs (from research section 8.1):**

1. **Cosmetics Brief:**
   - Brand: "花咲 Organic" (fictional organic skincare line)
   - Objective: "new_product"
   - Target: "30代女性、自然志向、敏感肌で悩む都市部在住の働く女性"
   - Platforms: ["instagram", "line"]
   - Product info: New organic face serum with Japanese botanical ingredients (椿油, 米ぬか)
   - Mood tags: ["natural", "luxury", "japanese"]
   - Brand has colors (soft green, cream), standard register, positioning statement

2. **Food/Restaurant Brief:**
   - Brand: "麺道 極" (fictional ramen shop)
   - Objective: "awareness"
   - Target: "渋谷エリアの20-40代のラーメン好き、仕事帰りのサラリーマン"
   - Platforms: ["x", "instagram"]
   - Product info: Grand opening of tonkotsu ramen shop in Shibuya, limited opening special
   - Mood tags: ["warm", "bold", "urban"]
   - Brand has minimal colors, casual register

3. **Fashion Brief:**
   - Brand: "Atelier SORA" (fictional young professional fashion)
   - Objective: "promotion"
   - Target: "25-35歳の若手社会人女性、オフィスカジュアルからプライベートまで"
   - Platforms: ["instagram", "x", "line"]
   - Product info: Spring collection launch, 10% off first purchase
   - Mood tags: ["modern", "bright", "minimal"]
   - Brand has colors (dusty rose, navy), standard register

4. **Personal Service (Nail Salon) Brief:**
   - Brand: "nail atelier nico" (fictional nail salon in Omotesando)
   - Objective: "engagement"
   - Target: "表参道エリアの20-30代女性、トレンドに敏感、SNS発信が好き"
   - Platforms: ["instagram", "line"]
   - Product info: New seasonal nail art designs, reservation campaign
   - Mood tags: ["pop", "colorful", "japanese"]
   - Brand has colors (pink, white), standard register

5. **Online Course Brief:**
   - Brand: "Global Bridge" (fictional business English coaching)
   - Objective: "conversion"
   - Target: "30-45歳の日本企業勤務ビジネスパーソン、英語でのプレゼン・会議に苦手意識"
   - Platforms: ["line", "x"]
   - Product info: 3-month intensive business English program, online, taught by bilingual instructor
   - Mood tags: ["modern", "professional"]
   - Brand has minimal colors, standard-to-formal register

**3 Edge Case Briefs:**

6. **Vague Brief (minimal information):**
   - Brand: "マイビジネス" (just a name)
   - Objective: "awareness"
   - Target: "" (empty)
   - Platforms: ["instagram"]
   - Product info: "うちのお店の宣伝をしたい" (just "I want to promote my shop")
   - Mood tags: [] (empty)
   - No brand colors, no positioning, no product catalog
   - **Test intent:** Strategic Insight must still classify confidently with low confidence scores

7. **Conflicting Brief:**
   - Brand: "LUXE BUDGET" (fictional)
   - Objective: "conversion"
   - Target: "高級志向の20代" (luxury-oriented 20s)
   - Platforms: ["instagram", "x"]
   - Product info: "最高級素材を使用した商品を1000円から" (premium materials at 1000 yen)
   - Mood tags: ["luxury", "casual"] (contradictory)
   - Brand register override: "casual" (conflicts with luxury positioning)
   - **Test intent:** Agents must handle contradictions gracefully, not crash

8. **Unusual Category (Pet Memorial):**
   - Brand: "にじの橋" (Rainbow Bridge — fictional pet memorial service)
   - Objective: "awareness"
   - Target: "ペットを亡くした30-60代、東京都内"
   - Platforms: ["line", "instagram"]
   - Product info: Pet memorial and cremation service with home pickup, grief counseling
   - Mood tags: ["natural", "japanese"]
   - Standard register, muted colors
   - **Test intent:** Emotionally sensitive category requires appropriate tone — not cheerful, not sales-y

Export:
- `STANDARD_TEST_BRIEFS: TestBrief[]` (5 briefs)
- `EDGE_CASE_BRIEFS: TestBrief[]` (3 briefs)
- `ALL_TEST_BRIEFS: TestBrief[]` (combined 8)

Where `TestBrief = { id: string, name: string, description: string, brief: CampaignBrief, brandProfile: BrandProfileForTest, testIntent: string }`.
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/ai/prompts/evaluation/test-briefs.ts` — no type errors. Verify 5 standard briefs + 3 edge cases = 8 total. Verify each brief has all required CampaignBrief fields. Verify edge cases have their specific test intent documented.
  </verify>
  <done>
8 test briefs exist covering cosmetics, food, fashion, personal service, online course (standard) plus vague, conflicting, and unusual category (edge cases). Each has a CampaignBrief, brand profile, and documented test intent. Edge cases specifically test sparse input handling, contradiction resolution, and emotional sensitivity.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create evaluation rubrics and naive prompt baselines for A/B comparison</name>
  <files>
    src/lib/ai/prompts/evaluation/rubrics.ts
    src/lib/ai/prompts/evaluation/naive-prompts.ts
  </files>
  <action>
**rubrics.ts:**

Create evaluation rubric definitions from research section 8.1 and 8.2.

Export `AGENT_RUBRIC_DIMENSIONS` — Record keyed by agent name, value is array of dimensions:

**Strategic Insight:**
- Classification Accuracy: Does the awareness level, desire, and framework match the brief's intent?
- Insight Depth: Is the target insight specific and actionable, not generic?
- Framework Appropriateness: Does the chosen framework suit the product type and channel mix?

**Creative Director:**
- Concept Clarity: Is the visual concept specific enough to generate imagery from?
- Visual Specificity: Are colors, composition, and mood concretely described?
- Downstream Actionability: Can a Copywriter and Art Director execute from this direction without ambiguity?

**Copywriter:**
- Japanese Naturalness: Does the copy read like native Japanese, not translationese?
- Register Correctness: Is the keigo level consistent and appropriate for the product category?
- Persuasiveness: Does the copy motivate action? Is the CTA compelling?
- Platform Fit: Does the copy match platform conventions (length, tone, hashtags)?

**Art Director:**
- Prompt Specificity: Is the prompt detailed enough for Flux to produce a specific image (not generic)?
- Realism Technique Inclusion: Are camera/lens, lighting, and skin realism keywords present?
- Category Awareness: Does the prompt reflect the product category's visual conventions?

**JP Localization:**
- Critique Accuracy: Are identified issues real problems (not false positives)?
- Suggestion Quality: Are suggested fixes natural Japanese that improve the copy?
- False Positive/Negative Rate: Does the agent approve good copy and reject bad copy correctly?

Each dimension has scoring anchors:
- 1 (Poor): {concrete example of what 1 looks like}
- 2 (Below Average): {example}
- 3 (Acceptable): {example}
- 4 (Good): {example}
- 5 (Excellent): {example}

Export `EVALUATION_RUBRICS` with the full rubric structure. Include improvement threshold: optimized prompts should average >= 0.5 points higher than naive across all dimensions (from research section 8.1).

**naive-prompts.ts:**

Create naive prompt builder functions representing Phase 9 defaults (before optimization). These are the baseline for A/B comparison.

Export 5 functions:

1. `buildNaiveStrategicInsightPrompt()` — Simple "You are a marketing analyst. Classify this brief." system prompt without framework knowledge, no XML tags, no few-shot examples, generic instructions.

2. `buildNaiveCreativeDirectorPrompt()` — Simple "You are a creative director. Generate a creative concept." without Japanese conventions, no upstream context parsing, no mode-dependent behavior.

3. `buildNaiveCopywriterPrompt()` — Mirror the existing `src/lib/ai/prompts/copy-generation.ts` buildSystemPrompt + buildCopyPrompt as-is. This IS the actual v1.0 prompt, serving as the naive baseline.

4. `buildNaiveArtDirectorPrompt()` — Mirror the existing `src/lib/ai/prompts/image-generation.ts` buildImagePrompt as-is. This IS the actual v1.0 prompt, serving as the naive baseline. Note: v1.0 has no camera/lens specs, no Raw Mode mention, no self-critique, no category awareness.

5. `buildNaiveJpLocalizationPrompt()` — Simple "You are a Japanese copy editor. Review this copy for quality." without weighted criteria, no structured issue format, no severity levels, no compliance flagging.

Each naive function should take the same inputs as the optimized version but produce significantly simpler prompts. The point is to demonstrate what "generic LLM prompting" looks like versus the research-backed optimized prompts.

Include JSDoc comments on each function noting: "NAIVE BASELINE — for A/B comparison only. Not used in production pipeline."
  </action>
  <verify>
Run `npx tsc --noEmit src/lib/ai/prompts/evaluation/rubrics.ts src/lib/ai/prompts/evaluation/naive-prompts.ts` — no type errors. Verify rubrics cover all 5 agents with scoring anchors. Verify naive prompts are genuinely simple (no XML tags, no framework knowledge, no self-critique). Verify naive Copywriter and Art Director mirror existing v1.0 code.
  </verify>
  <done>
Evaluation rubrics exist for all 5 agents with 1-5 scoring anchors per dimension and 0.5-point improvement threshold. Naive prompt baselines exist for all 5 agents — Copywriter and Art Director naive versions mirror actual v1.0 code; others are minimal generic prompts. A/B comparison infrastructure is complete: run any test brief through both naive and optimized prompts, score with rubrics, compare.
  </done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes for all 3 files
- 8 test briefs (5 standard + 3 edge) with full CampaignBrief structure
- Edge cases test specific failure modes (sparse input, contradictions, emotional sensitivity)
- Rubrics cover all 5 agents with concrete 1-5 scoring examples
- Naive prompts are genuinely simple vs. optimized prompts
- Naive Copywriter/Art Director mirror existing v1.0 code
- All requirement IDs appear in at least one plan across the phase
</verification>

<success_criteria>
- Test briefs enable reproducible evaluation across all 5 agents and 8 scenarios
- Rubrics provide objective scoring criteria (not subjective "does it feel good?")
- Naive vs. optimized comparison is fair (same inputs, same tool schemas, different prompts only)
- The evaluation framework supports the phase goal of "measurably superior output"
</success_criteria>

<output>
After completion, create `.planning/phases/09.1-agent-prompt-engineering-photorealistic-output/09.1-05-SUMMARY.md`
</output>
